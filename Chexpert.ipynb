{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Libraries and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import os\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the main csv file\n",
    "path = './data/'\n",
    "# Output directory for the sampled datasets\n",
    "output = './output/'\n",
    "# Base path for main dataset\n",
    "base_path = 'F:/'\n",
    "# Path where sampled dataset will be stored\n",
    "sampled_path = base_path + 'sampled'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(disease):\n",
    "    train_df = pd.read_csv(path + 'train.csv')\n",
    "    train_df[disease] = train_df[disease].fillna(0)\n",
    "    return train_df\n",
    "\n",
    "def create_sample_dataset(df, disease, sample_size):\n",
    "    df_positive = df[df[disease] == 1].sample(n=sample_size, random_state=42)\n",
    "    df_negative = df[df[disease] == 0].sample(n=sample_size, random_state=42)\n",
    "    df_sample = pd.concat([df_positive, df_negative])\n",
    "    df_sample = df_sample.sample(frac=1, random_state=42)\n",
    "    return df_sample\n",
    "\n",
    "def save_dataset(df, disease, approach, sample_size, iteration):\n",
    "    df_sample = create_sample_dataset(df, disease, sample_size)\n",
    "    #only save relevant columns\n",
    "    df_sample = df_sample[['Path', disease]]\n",
    "    df_sample.to_csv(f'{output}/{disease}_{approach}_{sample_size}_{iteration}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = ['U-Zeros', 'U-Ones']\n",
    "disease = 'Atelectasis'\n",
    "for approach in approaches:\n",
    "    train_df = get_dataset(disease)\n",
    "    train_df[disease] = train_df[disease].replace(-1,  0 if approach == 'U-Zeros' else 1)\n",
    "    sample_size = min(20000,train_df[disease].value_counts()[1])\n",
    "    no_of_samples = int(train_df[disease].size / sample_size)\n",
    "    for i in range(1, no_of_samples + 1):\n",
    "        save_dataset(train_df, disease, approach, sample_size, i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create New Sampled Dataset Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = 'U-Zeros'\n",
    "sample_size = 33376\n",
    "iteration = 4\n",
    "disease = 'Atelectasis'\n",
    "\n",
    "path_for_sampled = sampled_path + f'/{disease}_{approach}_{sample_size}_{iteration}'\n",
    "\n",
    "df = pd.read_csv(f'{output}/{disease}_{approach}_{sample_size}_{iteration}.csv')\n",
    "rows_processed = 0\n",
    "def move_files(row):\n",
    "    global rows_processed\n",
    "    print(\"Index: \", row['Path'])\n",
    "    path = row['Path']\n",
    "    if not os.path.exists(path_for_sampled):\n",
    "        os.makedirs(path_for_sampled)    \n",
    "    path_without_file = path[:path.rfind('/')]\n",
    "\n",
    "    if not os.path.exists(f'{path_for_sampled}/{path_without_file}'):\n",
    "        os.makedirs(f'{path_for_sampled}/{path_without_file}')\n",
    "\n",
    "    # shutil.copy(f'{base_path}/{path}', f'{path_for_sampled}/{path}')\n",
    "    # read the image, decrease jpeg quality and then copy it\n",
    "    image = tf.keras.preprocessing.image.load_img(f'{base_path}/{path}')\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = tf.image.encode_jpeg(image, quality=10)\n",
    "    tf.keras.preprocessing.image.save_img(f'{path_for_sampled}/{path}', image)\n",
    "    rows_processed += 1\n",
    "    print(\"Rows Processed: \", rows_processed)\n",
    "\n",
    "index = 0\n",
    "for _, row in df.iterrows():\n",
    "    print(\"Index: \", index)\n",
    "    move_files(row)\n",
    "    index += 1\n",
    "\n",
    "df.to_csv(f'{path_for_sampled}/train.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach = 'U-Zeros'\n",
    "sample_size = 33376\n",
    "iteration = 4\n",
    "\n",
    "dataset = pd.read_csv(f'{output}/{disease}_{approach}_{sample_size}_{iteration}.csv')\n",
    "\n",
    "train_df, val_df = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(train_df[disease].value_counts())\n",
    "print(val_df[disease].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, label):\n",
    "    img = tf.io.read_file(base_path + img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [224, 224])\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img, label\n",
    "\n",
    "def dataset_from_df(df, disease):\n",
    "    paths = df['Path'].values\n",
    "    labels = df[disease].values\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    ds = ds.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(64)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = dataset_from_df(val_df, disease)\n",
    "train_ds = dataset_from_df(train_df, disease)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=5, validation_data=test_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1ee1a62e867ff9aae35de182e396a92ac56e7ce2b9c877a0db8acfce3a00c3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
